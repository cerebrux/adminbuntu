#!/bin/bash
#
# backup-dirs-s3
#
# Backs up one or more directories to an Amazon AWS S3 bucket
#
#
# Copyright 2013 Andrew Ault
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software
# and associated documentation files (the "Software"), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute,
# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in all copies or
# substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT
# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#

usage (){
    echo " "
    echo Usage: backup-dirs-s3 directory1 [directory2...] s3destination
    echo Example: backup-dirs-s3 "directory/directory" "directory/another" "s3://destination-bucket"
    echo " "
}

if ! hash generate_rotate_filename 2>/dev/null; then
    echo "ERROR: generate_rotate_filename is required"
fi

append=`generate_rotate_filename`
ext=".tgz"
max_tarball_size=2000000000

# only root can run this script
if [ "$(id -u)" != "0" ]; then
    echo "This script must be run as root." 1>&2
fi

# array of all directories to back up with a trailing element containing the destination s3 bucket
directories=(${@})

# if fewer than 2 parameters, show usage
if [ ${#directories[@]} -lt 2 ]; then
    usage
    exit 1
fi

# clip off the s3 destination, retaining the rest of the array as a list of directories to backup
s3_destination=${directories[${#directories[@]} - 1]}
unset directories[${#directories[@]}-1]
directory_count=${#directories[@]}

# iterate directories array
for directory in "${directories[@]}" do
    if [ $directory = "/" ]; then
        echo "ERROR: Can't backup /"
        exit 1
    fi
    
    tarball_filename=$directory-${append}${ext}
    echo "tarball_filename: ${tarball_filename}"
    
    # change to parent directory
    echo "directory: ${directory}"
    cd $directory
    
    # get bare name of directory to back up
    bare_directory=${PWD##*/}
    echo "bare_directory: ${bare_directory}"
    
    cd ..
    parent_directory=`pwd`
    echo "parent_directory: ${parent_directory}"
    
    # create tarball
    tar -pczf $tarball_filename $bare_directory
    
    # limit tarball size uploaded to s3, if too big split tarball into chunks
    # upload result to s3 bucket
    tarball_filesize=$(stat -c%s "$tarball_filename")
    if (( $tarball_filesize > $max_tarball_size )); then
        echo "$tarball_filename is too large to upload to S3 at $tarball_filesize bytes"
        echo "Splitting $tarball_filename into 2Gb chunks"
        split -b10 $tarball_filename "$tarball_filename-"
        rm $tarball_filename
        s3cmd put *$ext* $s3bucket
        rm *"-${append}${ext}"* 2> /dev/null
    else
        echo "Uploading $tarball_filename"
        s3cmd put $tarball_filename $s3bucket
        rm $tarball_filename 2> /dev/null
    fi
    
done
echo "DONE"